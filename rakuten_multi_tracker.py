import os
import time
import csv
import logging
from datetime import datetime, timezone, timedelta
from playwright.sync_api import sync_playwright
from google.oauth2 import service_account
from googleapiclient.discovery import build

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('ranking_tracker.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# è¨­å®š
SHEET_ID = "1O_pT_RChITr7OvukkKVcjMC7aBji0xv2sr092Fn_FwE"  # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆID
SETTINGS_SHEET_NAME = "è¨­å®š"  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã‚’è¨˜è¼‰ã™ã‚‹ã‚·ãƒ¼ãƒˆå
RESULTS_SHEET_NAME = "ãƒ©ãƒ³ã‚­ãƒ³ã‚°å±¥æ­´"  # çµæœã‚’æ›¸ãè¾¼ã‚€ã‚·ãƒ¼ãƒˆå
CREDENTIALS_FILE = "credentials.json"  # èªè¨¼ãƒ•ã‚¡ã‚¤ãƒ«
USER_DATA_DIR = "./user_data_ranking"
CSV_FILE = "rakuten_ranking_auto.csv"
LAST_ROW_FILE = "last_row.txt"  # æœ€çµ‚æ›¸ãè¾¼ã¿è¡Œã‚’è¨˜éŒ²

def scrape_rankings(page, keyword, target_url, retry_count=0):
    """æ¥½å¤©ã§æ¤œç´¢ã—ã¦ã€PRé †ä½ã¨è‡ªç„¶æ¤œç´¢é †ä½ã‚’å–å¾—"""
    target_id = target_url.strip("/").split("/")[-1]
    search_url = f"https://search.rakuten.co.jp/search/mall/{keyword}/"
    logger.info(f"ğŸ” Searching for '{keyword}' (Target: {target_id})...")
    print(f"ğŸ” Searching for '{keyword}' (Target: {target_id})...")
    
    try:
        page.goto(search_url, timeout=90000)
        page.wait_for_load_state("domcontentloaded", timeout=60000)
        time.sleep(3)
        page.wait_for_selector(".searchresultitem", timeout=30000)
    except Exception as e:
        logger.error(f"âš ï¸ Search page error for '{keyword}': {e}")
        print(f"âš ï¸ Search page error for '{keyword}': {e}")
        if retry_count < 1:
            logger.info(f"   â†’ Retrying in 5 seconds...")
            print(f"   â†’ Retrying in 5 seconds...")
            time.sleep(5)
            return scrape_rankings(page, keyword, target_url, retry_count + 1)
        return "ERROR", "ERROR"

    items = page.locator(".searchresultitem").all()
    pr_count = 0
    organic_count = 0
    target_pr_rank = None
    target_organic_rank = None
    
    for item in items:
        doc_type = item.get_attribute("data-track-doc-type")
        is_pr = (doc_type == "rpp")
        if is_pr: 
            pr_count += 1
        else: 
            organic_count += 1
            
        is_match = False
        
        # 1. Check data-track-variantid
        variant_id = item.get_attribute("data-track-variantid") or ""
        if target_id in variant_id:
            is_match = True
            
        # 2. Check all link hrefs
        if not is_match:
            links = item.locator("a").all()
            for link in links:
                if target_id in (link.get_attribute("href") or ""):
                    is_match = True
                    break
        
        # 3. Check all image sources
        if not is_match:
            imgs = item.locator("img").all()
            for img in imgs:
                if target_id in (img.get_attribute("src") or ""):
                    is_match = True
                    break
        
        if is_match:
            if is_pr and target_pr_rank is None:
                target_pr_rank = pr_count
                logger.info(f"   âœ¨ Found in PR (RPP)! Rank: {target_pr_rank}")
                print(f"   âœ¨ Found in PR (RPP)! Rank: {target_pr_rank}")
            elif not is_pr and target_organic_rank is None:
                target_organic_rank = organic_count
                logger.info(f"   âœ¨ Found in Organic! Rank: {target_organic_rank}")
                print(f"   âœ¨ Found in Organic! Rank: {target_organic_rank}")

    pr_result = target_pr_rank or "åœå¤–"
    organic_result = target_organic_rank or "åœå¤–"
    logger.info(f"   Result: PR={pr_result}, Organic={organic_result}")
    return pr_result, organic_result

def get_sheets_service():
    """Google Sheets APIã‚µãƒ¼ãƒ“ã‚¹ã‚’å–å¾—"""
    creds = service_account.Credentials.from_service_account_file(
        CREDENTIALS_FILE,
        scopes=['https://www.googleapis.com/auth/spreadsheets']
    )
    service = build('sheets', 'v4', credentials=creds)
    return service

def load_keywords_from_sheet(service):
    """ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã‚€"""
    try:
        # è¨­å®šã‚·ãƒ¼ãƒˆã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€ï¼ˆ2è¡Œç›®ä»¥é™ã€Aåˆ—ã¨Båˆ—ï¼‰
        range_name = f'{SETTINGS_SHEET_NAME}!A2:B'
        result = service.spreadsheets().values().get(
            spreadsheetId=SHEET_ID,
            range=range_name
        ).execute()
        
        values = result.get('values', [])
        
        if not values:
            logger.warning(f"âš ï¸ No keywords found in '{SETTINGS_SHEET_NAME}' sheet")
            return []
        
        keywords_list = []
        for row in values:
            # ç©ºè¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
            if len(row) >= 2 and row[0].strip() and row[1].strip():
                keywords_list.append({
                    "keyword": row[0].strip(),
                    "url": row[1].strip()
                })
        
        logger.info(f"ğŸ“‹ Loaded {len(keywords_list)} keywords from spreadsheet")
        print(f"ğŸ“‹ Loaded {len(keywords_list)} keywords from spreadsheet")
        return keywords_list
        
    except Exception as e:
        logger.error(f"âŒ Error loading keywords from spreadsheet: {e}")
        print(f"âŒ Error loading keywords from spreadsheet: {e}")
        return []

def write_to_sheets(service, start_row, data):
    """Google Sheetsã«ä¸€æ‹¬æ›¸ãè¾¼ã¿"""
    try:
        # ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
        values = []
        for row_data in data:
            values.append([
                row_data['date'],
                row_data['keyword'],
                row_data['url'],
                str(row_data['pr']),
                str(row_data['organic'])
            ])
        
        # æ›¸ãè¾¼ã¿ç¯„å›²ã‚’æŒ‡å®šï¼ˆçµæœã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã‚€ï¼‰
        range_name = f'{RESULTS_SHEET_NAME}!A{start_row}:E{start_row + len(values) - 1}'
        
        body = {
            'values': values
        }
        
        result = service.spreadsheets().values().update(
            spreadsheetId=SHEET_ID,
            range=range_name,
            valueInputOption='RAW',
            body=body
        ).execute()
        
        logger.info(f"âœ… Successfully wrote {result.get('updatedCells')} cells to spreadsheet")
        print(f"âœ… Successfully wrote {result.get('updatedCells')} cells to spreadsheet")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Error writing to spreadsheet: {e}")
        print(f"âŒ Error writing to spreadsheet: {e}")
        return False

def update_spreadsheet():
    start_time = datetime.now()
    logger.info(f"\n{'='*60}")
    logger.info(f"ğŸš€ Starting Ranking Check at {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"{'='*60}")
    print(f"\nğŸš€ Starting Ranking Check at {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã‚€
    try:
        service = get_sheets_service()
        keywords_list = load_keywords_from_sheet(service)
        
        if not keywords_list:
            logger.error("âŒ No keywords to process. Exiting.")
            print("âŒ No keywords to process. Please check the settings sheet.")
            return
    except Exception as e:
        logger.error(f"âŒ Failed to load keywords: {e}")
        print(f"âŒ Failed to load keywords: {e}")
        return
    
    logger.info(f"Total keywords to process: {len(keywords_list)}")
    
    # æœ€çµ‚æ›¸ãè¾¼ã¿è¡Œã‚’èª­ã¿è¾¼ã‚“ã§æ¬¡ã®è¡Œã‹ã‚‰è¿½è¨˜
    if os.path.exists(LAST_ROW_FILE):
        with open(LAST_ROW_FILE, "r") as f:
            last_row = int(f.read().strip())
            start_row = last_row + 1
        logger.info(f"Last written row: {last_row}, starting from row {start_row}")
    else:
        start_row = 2  # åˆå›ã¯2è¡Œç›®ã‹ã‚‰ï¼ˆ1è¡Œç›®ã¯ãƒ˜ãƒƒãƒ€ãƒ¼ï¼‰
        logger.info(f"First run - starting from row {start_row}")

    with sync_playwright() as p:
        # CIç’°å¢ƒï¼ˆGitHub Actionsï¼‰ã¾ãŸã¯Docker headlessãƒ¢ãƒ¼ãƒ‰ã‹ã©ã†ã‹ã§headlessè¨­å®šã‚’å¤‰æ›´
        is_ci = os.environ.get('CI') == 'true'
        
        context = p.chromium.launch_persistent_context(
            user_data_dir=USER_DATA_DIR,
            headless=is_ci, # CIç’°å¢ƒãªã‚‰Trueï¼ˆç”»é¢ãªã—ï¼‰ã€ãƒ­ãƒ¼ã‚«ãƒ«ãªã‚‰Falseï¼ˆç”»é¢ã‚ã‚Šï¼‰
            args=["--disable-blink-features=AutomationControlled"]
        )
        
        # 1. é †ä½å–å¾—
        logger.info("ğŸ” Scraping rankings...")
        print("ğŸ” Scraping rankings...")
        all_results = []

        
        # æ—¥æœ¬æ™‚é–“ï¼ˆJST = UTC+9ï¼‰ã§æ—¥æ™‚ã‚’å–å¾—
        # GitHub Actionsã¯UTCç’°å¢ƒãªã®ã§ã€æ˜ç¤ºçš„ã«9æ™‚é–“åŠ ç®—
        jst_time = datetime.utcnow() + timedelta(hours=9)
        current_date = jst_time.strftime("%Y-%m-%d %H:%M")
        
        for idx, task in enumerate(keywords_list, 1):
            logger.info(f"Processing keyword {idx}/{len(keywords_list)}: {task['keyword']}")
            # Create new page for each keyword to prevent memory leaks
            try:
                page = context.new_page()
                pr, organic = scrape_rankings(page, task["keyword"], task["url"])
            except Exception as e:
                logger.error(f"âŒ Error processing {task['keyword']}: {e}")
                print(f"âŒ Error processing {task['keyword']}: {e}")
                pr, organic = "ERROR", "ERROR"
            finally:
                if 'page' in locals():
                    page.close()

            all_results.append({
                "date": current_date,
                "keyword": task["keyword"],
                "url": task["url"],
                "pr": pr,
                "organic": organic
            })
        

        context.close()
        logger.info(f"âœ… Scraping completed. Total results: {len(all_results)}")
    
    # 2. Google Sheetsã«æ›¸ãè¾¼ã¿
    logger.info("ğŸ“Š Writing to Google Sheets...")
    print("ğŸ“Š Writing to Google Sheets...")
    
    try:
        # serviceã¯æ—¢ã«å–å¾—æ¸ˆã¿ãªã®ã§å†åˆ©ç”¨
        success = write_to_sheets(service, start_row, all_results)
        
        if success:
            # æœ€çµ‚æ›¸ãè¾¼ã¿è¡Œã‚’ä¿å­˜ï¼ˆæ¬¡å›ã¯ã“ã®æ¬¡ã®è¡Œã‹ã‚‰è¿½è¨˜ï¼‰
            final_row = start_row + len(all_results) - 1
            with open(LAST_ROW_FILE, "w") as f:
                f.write(str(final_row))
            logger.info(f"ğŸ“ Data written to rows {start_row} to {final_row}")
            logger.info(f"ğŸ“ Next run will start from row {final_row + 1}")
            print(f"ğŸ“ Data written to rows {start_row} to {final_row}")
    except Exception as e:
        logger.error(f"âŒ Failed to write to Google Sheets: {e}")
        print(f"âŒ Failed to write to Google Sheets: {e}")
    
    # 3. CSVä¿å­˜
    save_to_csv(all_results)
    
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()
    logger.info(f"\n{'='*60}")
    logger.info(f"âœ… All rankings updated in the Spreadsheet and CSV.")
    logger.info(f"Total execution time: {duration:.2f} seconds")
    logger.info(f"Completed at: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"{'='*60}\n")
    print(f"\nâœ… All rankings updated! Execution time: {duration:.2f} seconds")

def save_to_csv(results):
    file_exists = os.path.isfile(CSV_FILE)
    with open(CSV_FILE, "a", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(f, fieldnames=["date", "keyword", "url", "pr", "organic"])
        if not file_exists:
            writer.writeheader()
        writer.writerows(results)

if __name__ == "__main__":
    update_spreadsheet()
